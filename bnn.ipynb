{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlnxgySGKhcQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agJ95gwnKhcS"
      },
      "source": [
        "\n",
        "# Example: Bayesian Neural Network\n",
        "\n",
        "We demonstrate how to use NUTS to do inference on a simple (small)\n",
        "Bayesian neural network with two hidden layers.\n",
        "\n",
        "<img src=\"file://../_static/img/examples/bnn.png\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpyro"
      ],
      "metadata": {
        "id": "IH6yJQRiHDqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d866e9-53ae-45a8-c034-a828b84e37e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpyro\n",
            "  Downloading numpyro-0.12.1-py3-none-any.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from numpyro) (0.4.13)\n",
            "Requirement already satisfied: jaxlib>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from numpyro) (0.4.13+cuda11.cudnn86)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from numpyro) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from numpyro) (4.65.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->numpyro) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->numpyro) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->numpyro) (1.10.1)\n",
            "Installing collected packages: numpyro\n",
            "Successfully installed numpyro-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from jax import vmap\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "import numpyro\n",
        "from numpyro import handlers\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "\n",
        "matplotlib.use(\"Agg\")  # noqa: E402"
      ],
      "metadata": {
        "id": "uzU4-oM3O14_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the non-linearity we use in our neural network\n",
        "def nonlin(x):\n",
        "    return jnp.tanh(x)\n",
        "\n",
        "\n",
        "# a two-layer bayesian neural network with computational flow\n",
        "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
        "# hidden units. (note we indicate tensor dimensions in the comments)\n",
        "def model(X, Y, D_H, D_Y=1):\n",
        "    N, D_X = X.shape\n",
        "\n",
        "    # sample first layer (we put unit normal priors on all weights)\n",
        "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H))))\n",
        "    assert w1.shape == (D_X, D_H)\n",
        "    z1 = nonlin(jnp.matmul(X, w1))  # <= first layer of activations\n",
        "    assert z1.shape == (N, D_H)\n",
        "\n",
        "    # sample second layer\n",
        "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H))))\n",
        "    assert w2.shape == (D_H, D_H)\n",
        "    z2 = nonlin(jnp.matmul(z1, w2))  # <= second layer of activations\n",
        "    assert z2.shape == (N, D_H)\n",
        "\n",
        "    # sample final layer of weights and neural network output\n",
        "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y))))\n",
        "    assert w3.shape == (D_H, D_Y)\n",
        "    z3 = jnp.matmul(z2, w3)  # <= output of the neural network\n",
        "    assert z3.shape == (N, D_Y)\n",
        "\n",
        "    if Y is not None:\n",
        "        assert z3.shape == Y.shape\n",
        "\n",
        "    # we put a prior on the observation noise\n",
        "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
        "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
        "\n",
        "    # observe data\n",
        "    with numpyro.plate(\"data\", N):\n",
        "        # note we use to_event(1) because each observation has shape (1,)\n",
        "        numpyro.sample(\"Y\", dist.Normal(z3, sigma_obs).to_event(1), obs=Y)\n",
        "\n",
        "\n",
        "# helper function for HMC inference\n",
        "def run_inference(model, num_warmups, num_sample, num_chain, rng_key, X, Y, D_H):\n",
        "    start = time.time()\n",
        "    kernel = NUTS(model)\n",
        "    mcmc = MCMC(\n",
        "        kernel,\n",
        "        num_warmup=num_warmups,\n",
        "        num_samples=num_sample,\n",
        "        num_chains=num_chain,\n",
        "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
        "    )\n",
        "    mcmc.run(rng_key, X, Y, D_H)\n",
        "    mcmc.print_summary()\n",
        "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
        "    return mcmc.get_samples()\n",
        "\n",
        "\n",
        "# helper function for prediction\n",
        "def predict(model, rng_key, samples, X, D_H):\n",
        "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
        "    # note that Y will be sampled in the model because we pass Y=None here\n",
        "    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
        "    return model_trace[\"Y\"][\"value\"]\n",
        "\n",
        "\n",
        "# create artificial regression dataset\n",
        "def get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n",
        "    D_Y = 1  # create 1d outputs\n",
        "    X = jnp.linspace(-1, 1, N)\n",
        "    X = jnp.power(X[:, np.newaxis], jnp.arange(D_X))\n",
        "    W = 0.5 * np.random.randn(D_X)\n",
        "    Y = jnp.dot(X, W) + 0.5 * jnp.power(0.5 + X[:, 1], 2.0) * jnp.sin(4.0 * X[:, 1])\n",
        "    Y += sigma_obs * np.random.randn(N)\n",
        "    Y = Y[:, np.newaxis]\n",
        "    Y -= jnp.mean(Y)\n",
        "    Y /= jnp.std(Y)\n",
        "\n",
        "    assert X.shape == (N, D_X)\n",
        "    assert Y.shape == (N, D_Y)\n",
        "\n",
        "    X_test = jnp.linspace(-1.3, 1.3, N_test)\n",
        "    X_test = jnp.power(X_test[:, np.newaxis], jnp.arange(D_X))\n",
        "\n",
        "    return X, Y, X_test"
      ],
      "metadata": {
        "id": "bCHZ2l6eO4xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99so-j0HKhcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18a84f7-970f-4137-ec3e-60e513131e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "sample: 100%|██████████| 3000/3000 [01:18<00:00, 38.27it/s, 1023 steps of size 3.29e-03. acc. prob=0.93]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
            "  prec_obs     23.66      3.60     23.59     17.88     29.36   2087.50      1.00\n",
            "   w1[0,0]     -0.07      1.13     -0.12     -1.81      1.83    268.34      1.00\n",
            "   w1[0,1]     -0.12      1.06     -0.18     -1.95      1.53    296.29      1.00\n",
            "   w1[0,2]      0.00      1.10      0.00     -1.89      1.67    240.51      1.01\n",
            "   w1[0,3]     -0.05      1.07     -0.09     -1.66      1.88    243.34      1.00\n",
            "   w1[0,4]     -0.00      1.01     -0.00     -1.80      1.56    322.65      1.00\n",
            "   w1[1,0]      0.08      1.15      0.17     -1.80      1.73    227.86      1.00\n",
            "   w1[1,1]      0.17      1.17      0.28     -1.67      1.98    199.40      1.00\n",
            "   w1[1,2]     -0.10      1.15     -0.15     -1.81      1.65    165.09      1.00\n",
            "   w1[1,3]      0.06      1.14      0.06     -1.60      1.98    235.55      1.01\n",
            "   w1[1,4]      0.11      1.18      0.15     -1.74      1.98    191.82      1.01\n",
            "   w1[2,0]      0.03      1.00      0.01     -1.49      1.74    240.37      1.00\n",
            "   w1[2,1]      0.20      1.03      0.18     -1.48      1.93    299.39      1.00\n",
            "   w1[2,2]     -0.04      0.98     -0.05     -1.78      1.50    169.83      1.00\n",
            "   w1[2,3]      0.06      1.02      0.08     -1.48      1.87    224.87      1.00\n",
            "   w1[2,4]      0.10      1.06      0.08     -1.64      1.84    190.54      1.00\n",
            "   w2[0,0]     -0.01      1.02     -0.02     -1.62      1.68    928.88      1.00\n",
            "   w2[0,1]     -0.03      1.03     -0.05     -1.72      1.63    729.77      1.01\n",
            "   w2[0,2]     -0.00      1.03      0.07     -1.72      1.65    776.66      1.00\n",
            "   w2[0,3]     -0.01      1.06     -0.02     -1.67      1.72    944.60      1.00\n",
            "   w2[0,4]     -0.02      1.05     -0.03     -1.66      1.77    747.64      1.00\n",
            "   w2[1,0]      0.04      1.03      0.04     -1.62      1.75    954.18      1.00\n",
            "   w2[1,1]     -0.02      1.09     -0.01     -1.80      1.68   1034.92      1.00\n",
            "   w2[1,2]     -0.05      1.07     -0.07     -1.58      1.84    943.18      1.00\n",
            "   w2[1,3]     -0.01      1.05      0.00     -1.79      1.62    915.84      1.00\n",
            "   w2[1,4]     -0.03      1.06     -0.05     -1.82      1.63    929.12      1.00\n",
            "   w2[2,0]      0.04      1.02      0.04     -1.57      1.68    864.54      1.00\n",
            "   w2[2,1]     -0.02      1.06     -0.07     -1.92      1.56    917.95      1.00\n",
            "   w2[2,2]     -0.03      1.05     -0.04     -1.67      1.72    838.37      1.00\n",
            "   w2[2,3]      0.01      1.06      0.03     -1.89      1.55    760.44      1.00\n",
            "   w2[2,4]      0.03      1.05      0.05     -1.73      1.69    778.39      1.00\n",
            "   w2[3,0]     -0.04      1.05     -0.03     -1.51      1.79    991.09      1.00\n",
            "   w2[3,1]     -0.04      1.02     -0.07     -1.66      1.64    992.41      1.00\n",
            "   w2[3,2]     -0.01      1.04      0.01     -1.75      1.65   1047.97      1.00\n",
            "   w2[3,3]      0.05      1.01      0.09     -1.58      1.65    912.67      1.00\n",
            "   w2[3,4]      0.00      1.04     -0.01     -1.69      1.68    888.56      1.00\n",
            "   w2[4,0]      0.05      1.04      0.02     -1.59      1.71    899.16      1.00\n",
            "   w2[4,1]     -0.00      1.04      0.02     -1.70      1.68    953.15      1.00\n",
            "   w2[4,2]      0.02      1.03      0.01     -1.68      1.64   1168.31      1.00\n",
            "   w2[4,3]     -0.04      1.05     -0.04     -1.93      1.55   1010.76      1.00\n",
            "   w2[4,4]      0.04      1.04      0.06     -1.50      1.87   1007.25      1.00\n",
            "   w3[0,0]      0.00      1.45     -0.03     -2.29      2.33    317.26      1.00\n",
            "   w3[1,0]      0.08      1.52      0.09     -2.56      2.37    398.72      1.00\n",
            "   w3[2,0]      0.09      1.48      0.10     -2.34      2.45    366.91      1.00\n",
            "   w3[3,0]      0.01      1.50     -0.02     -2.30      2.40    307.95      1.01\n",
            "   w3[4,0]      0.03      1.50      0.02     -2.52      2.26    279.84      1.01\n",
            "\n",
            "Number of divergences: 6\n",
            "\n",
            "MCMC elapsed time: 87.78163504600525\n"
          ]
        }
      ],
      "source": [
        "num_data = 100\n",
        "num_hidden = 5\n",
        "num_samples = 2000\n",
        "num_chains = 1\n",
        "num_warmup = 1000\n",
        "\n",
        "N, D_X, D_H = num_data, 3, num_hidden\n",
        "X, Y, X_test = get_data(N=N, D_X=D_X)\n",
        "\n",
        "# do inference\n",
        "rng_key, rng_key_predict = random.split(random.PRNGKey(int(time.time())))\n",
        "samples = run_inference(model, num_warmup, num_samples, num_chains, rng_key, X, Y, D_H)\n",
        "\n",
        "# predict Y_test at inputs X_test\n",
        "vmap_args = (\n",
        "    samples,\n",
        "    random.split(rng_key_predict, num_samples * num_chains),\n",
        ")\n",
        "predictions_complete = vmap(\n",
        "    lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H)\n",
        ")(*vmap_args)\n",
        "predictions = predictions_complete[..., 0]\n",
        "\n",
        "# compute mean prediction and confidence interval around median\n",
        "mean_prediction = jnp.mean(predictions, axis=0)\n",
        "percentiles = np.percentile(predictions, [0.5, 99.5], axis=0)\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
        "# plot 90% confidence level of predictions\n",
        "ax.fill_between(\n",
        "    X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
        ")\n",
        "# plot mean prediction\n",
        "ax.plot(X_test[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Media e intervalo de confianza del 99%\")\n",
        "\n",
        "plt.savefig(\"bnn_plot.pdf\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}