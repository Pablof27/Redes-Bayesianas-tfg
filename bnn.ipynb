{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por Pablo Fornet Martín\n",
        "Basado en el código de https://num.pyro.ai/en/stable/examples/bnn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzU4-oM3O14_"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from jax import vmap\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "import numpyro\n",
        "from numpyro import handlers\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "\n",
        "matplotlib.use(\"Agg\")  # noqa: E402"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCHZ2l6eO4xm"
      },
      "outputs": [],
      "source": [
        "# the non-linearity we use in our neural network\n",
        "def nonlin(x):\n",
        "    return jnp.tanh(x)\n",
        "\n",
        "\n",
        "# a two-layer bayesian neural network with computational flow\n",
        "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
        "# hidden units. (note we indicate tensor dimensions in the comments)\n",
        "def model(X, Y, D_H, D_Y=1):\n",
        "    N, D_X = X.shape\n",
        "\n",
        "    # sample first layer (we put unit normal priors on all weights)\n",
        "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H))))\n",
        "    assert w1.shape == (D_X, D_H)\n",
        "    z1 = nonlin(jnp.matmul(X, w1))  # <= first layer of activations\n",
        "    assert z1.shape == (N, D_H)\n",
        "\n",
        "    # sample second layer\n",
        "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H))))\n",
        "    assert w2.shape == (D_H, D_H)\n",
        "    z2 = nonlin(jnp.matmul(z1, w2))  # <= second layer of activations\n",
        "    assert z2.shape == (N, D_H)\n",
        "\n",
        "    # sample final layer of weights and neural network output\n",
        "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y))))\n",
        "    assert w3.shape == (D_H, D_Y)\n",
        "    z3 = jnp.matmul(z2, w3)  # <= output of the neural network\n",
        "    assert z3.shape == (N, D_Y)\n",
        "\n",
        "    if Y is not None:\n",
        "        assert z3.shape == Y.shape\n",
        "\n",
        "    # we put a prior on the observation noise\n",
        "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
        "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
        "\n",
        "    # observe data\n",
        "    with numpyro.plate(\"data\", N):\n",
        "        # note we use to_event(1) because each observation has shape (1,)\n",
        "        numpyro.sample(\"Y\", dist.Normal(z3, sigma_obs).to_event(1), obs=Y)\n",
        "\n",
        "\n",
        "# helper function for HMC inference\n",
        "def run_inference(model, num_warmups, num_sample, num_chain, rng_key, X, Y, D_H):\n",
        "    start = time.time()\n",
        "    kernel = NUTS(model)\n",
        "    mcmc = MCMC(\n",
        "        kernel,\n",
        "        num_warmup=num_warmups,\n",
        "        num_samples=num_sample,\n",
        "        num_chains=num_chain,\n",
        "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
        "    )\n",
        "    mcmc.run(rng_key, X, Y, D_H)\n",
        "    mcmc.print_summary()\n",
        "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
        "    return mcmc.get_samples()\n",
        "\n",
        "\n",
        "# helper function for prediction\n",
        "def predict(model, rng_key, samples, X, D_H):\n",
        "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
        "    # note that Y will be sampled in the model because we pass Y=None here\n",
        "    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
        "    return model_trace[\"Y\"][\"value\"]\n",
        "\n",
        "\n",
        "# create artificial regression dataset\n",
        "def get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n",
        "    D_Y = 1  # create 1d outputs\n",
        "    X = jnp.linspace(-1, 1, N)\n",
        "    X = jnp.power(X[:, np.newaxis], jnp.arange(D_X))\n",
        "    W = 0.5 * np.random.randn(D_X)\n",
        "    Y = jnp.dot(X, W) + 0.5 * jnp.power(0.5 + X[:, 1], 2.0) * jnp.sin(4.0 * X[:, 1])\n",
        "    Y += sigma_obs * np.random.randn(N)\n",
        "    Y = Y[:, np.newaxis]\n",
        "    Y -= jnp.mean(Y)\n",
        "    Y /= jnp.std(Y)\n",
        "\n",
        "    assert X.shape == (N, D_X)\n",
        "    assert Y.shape == (N, D_Y)\n",
        "\n",
        "    X_test = jnp.linspace(-1.3, 1.3, N_test)\n",
        "    X_test = jnp.power(X_test[:, np.newaxis], jnp.arange(D_X))\n",
        "\n",
        "    return X, Y, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99so-j0HKhcT",
        "outputId": "d18a84f7-970f-4137-ec3e-60e513131e59"
      },
      "outputs": [],
      "source": [
        "num_data = 100\n",
        "num_hidden = 5\n",
        "num_samples = 2000\n",
        "num_chains = 1\n",
        "num_warmup = 1000\n",
        "\n",
        "N, D_X, D_H = num_data, 3, num_hidden\n",
        "X, Y, X_test = get_data(N=N, D_X=D_X)\n",
        "\n",
        "# do inference\n",
        "rng_key, rng_key_predict = random.split(random.PRNGKey(int(time.time())))\n",
        "samples = run_inference(model, num_warmup, num_samples, num_chains, rng_key, X, Y, D_H)\n",
        "\n",
        "# predict Y_test at inputs X_test\n",
        "vmap_args = (\n",
        "    samples,\n",
        "    random.split(rng_key_predict, num_samples * num_chains),\n",
        ")\n",
        "predictions_complete = vmap(\n",
        "    lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H)\n",
        ")(*vmap_args)\n",
        "predictions = predictions_complete[..., 0]\n",
        "\n",
        "# compute mean prediction and confidence interval around median\n",
        "mean_prediction = jnp.mean(predictions, axis=0)\n",
        "percentiles = np.percentile(predictions, [0.5, 99.5], axis=0)\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
        "# plot 90% confidence level of predictions\n",
        "ax.fill_between(\n",
        "    X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
        ")\n",
        "# plot mean prediction\n",
        "ax.plot(X_test[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Media e intervalo de confianza del 99%\")\n",
        "\n",
        "plt.savefig(\"bnn_plot.pdf\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
